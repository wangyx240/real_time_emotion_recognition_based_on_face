{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    N      EN    \n",
    "    \n",
    "    0      neutral     \n",
    "    1      calm        \n",
    "    2      happy       \n",
    "    3      sad         \n",
    "    4      angry       \n",
    "    5      fearful     \n",
    "    6      disgust    \n",
    "    7      surprised  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from imutils.face_utils import FaceAligner\n",
    "import dlib\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# progress bar\n",
    "def update_progress(progress):\n",
    "    bar_length = 100\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait=True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format(\n",
    "        \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_directory = os.path.dirname(os.getcwd())\n",
    "\n",
    "# ___ IMAGE FILTER DATA ___\n",
    "\n",
    "# folder index processing will start from (lower = more instances)\n",
    "folder_start_index = 0\n",
    "\n",
    "# skip <skip> frames after reading 1 frame (to simulate lower fps) (lower = more instances)\n",
    "skip = 0\n",
    "\n",
    "# instances max count for each emotion in the result dataset (lower = less instances)\n",
    "max_count = 1000\n",
    "\n",
    "# frames count in one instance in the result dataset (lower = more instances but they are smaller)\n",
    "frames_count = 30\n",
    "\n",
    "# number of emotion classes\n",
    "emotions_count = 8\n",
    "emotions = np.zeros(emotions_count)\n",
    "\n",
    "# frames max count to get from each folder with frames (lower = less instances)\n",
    "max_from_folder = 1000\n",
    "\n",
    "# resolution of the result frames that will be in the result dataset\n",
    "frames_resolution = [64, 64, 1]\n",
    "\n",
    "# frame index in each folder processing will start from (lower = more instances)\n",
    "image_start_index = 0\n",
    "\n",
    "# instances max count from one folder with frames (each instance has <frames_count> images) (lower = less instances)\n",
    "max_array_count = 1000\n",
    "\n",
    "# saves result pickle data every <save_index> processed folder\n",
    "save_index = 500\n",
    "\n",
    "# ___ FACE DETECTOR ___\n",
    "\n",
    "# threshold for face detector\n",
    "conf_threshold = 0.8\n",
    "\n",
    "modelFile = os.path.join(project_directory, \"models\\\\opencv_face_detector_uint8.pb\")\n",
    "configFile = os.path.join(project_directory, \"models\\\\opencv_face_detector.pbtxt\")\n",
    "face_detector = cv2.dnn.readNetFromTensorflow(modelFile, configFile)\n",
    "\n",
    "# ___ FACE ALIGNER ___ (uses emotion recognition model input shape)\n",
    "predictor = dlib.shape_predictor(os.path.join(project_directory, \"models\\\\shape_predictor_68_face_landmarks.dat\"))\n",
    "fa = FaceAligner(predictor, desiredFaceWidth=frames_resolution[0], desiredFaceHeight=frames_resolution[1])\n",
    "\n",
    "def toOneHot(number, emotions_count):\n",
    "    arr = np.zeros((1, emotions_count), dtype=int)\n",
    "    arr[0][number] = 1\n",
    "    return arr[0]\n",
    "\n",
    "# ___ FOLDERS ___\n",
    "training_frames_folder = os.path.join(project_directory, \"dataset\\\\TrainingFrames\")\n",
    "test_frames_folder = os.path.join(project_directory, \"dataset\\\\TestFrames\")\n",
    "\n",
    "training_save_folder = os.path.join(project_directory, \"dataset\\\\resolution_{}x{}x{}_train.pickle\".format(frames_resolution[0], frames_resolution[1], frames_resolution[2]))\n",
    "test_save_folder = os.path.join(project_directory, \"dataset\\\\resolution_{}x{}x{}_test.pickle\".format(frames_resolution[0], frames_resolution[1], frames_resolution[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result data\n",
    "data = []\n",
    "# with open(\"dataset//resolution_{}x{}x{}_index_{}_frames_count_{}_skip_{}_train.pickle\".format(frames_resolution[0], frames_resolution[1], frames_resolution[2], i, frames_count, skip), 'rb') as handle:\n",
    "#     data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data for test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_data(frames_folder, save_folder):\n",
    "    frames_folders = sorted(os.listdir(frames_folder), key=int)\n",
    "    \n",
    "    for i, folder_name in enumerate(frames_folders):\n",
    "        if i < folder_start_index:\n",
    "            continue\n",
    "\n",
    "        if(i%save_index==0 and i!=0):\n",
    "            with open(os.path.join(project_directory, save_folder), 'wb') as handle:\n",
    "                pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        folder_path = os.path.join(*(frames_folder, folder_name))  \n",
    "\n",
    "        # load emotion number for the annotation file in the folder\n",
    "        with open(os.path.join(*(folder_path, \"annotation.pickle\")), 'rb') as handle:\n",
    "            annotation = pickle.load(handle)\n",
    "\n",
    "        # if arrays count in the result data for this emotion is less than max_count\n",
    "        emotion = annotation[\"emotion\"]\n",
    "\n",
    "        # if directory exist and contain enough frames for 1 array \n",
    "        if (os.path.exists(folder_path)) and (len(os.listdir(folder_path)) - 1 >= frames_count):\n",
    "\n",
    "            if(emotions[emotion] < max_count):\n",
    "\n",
    "                # loop through all frames\n",
    "                length = len(os.listdir(folder_path)) - 1\n",
    "                array_count = 0\n",
    "                index = image_start_index\n",
    "                images = []\n",
    "                while (index < length) and (max_from_folder > array_count) :\n",
    "                    image_path = os.path.join(folder_path, \"frame_{}.png\".format(index))\n",
    "                    image = cv2.imread(image_path, 1)\n",
    "\n",
    "                    # blob for face detector\n",
    "                    blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300), [104, 117, 123], False, False)\n",
    "                    face_detector.setInput(blob)\n",
    "                    faces = face_detector.forward()\n",
    "\n",
    "                    # loop through all found faces\n",
    "                    for f in range(faces.shape[2]):\n",
    "                        confidence = faces[0, 0, f, 2]\n",
    "                        if confidence > conf_threshold:\n",
    "                            x1 = int(faces[0, 0, f, 3] * image.shape[1])\n",
    "                            y1 = int(faces[0, 0, f, 4] * image.shape[0])\n",
    "                            x2 = int(faces[0, 0, f, 5] * image.shape[1])\n",
    "                            y2 = int(faces[0, 0, f, 6] * image.shape[0])\n",
    "\n",
    "                            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                            detected_face = fa.align(image, gray, dlib.rectangle(left=x1, top=y1, right=x2, bottom=y2))\n",
    "                            if detected_face.size != 0:\n",
    "\n",
    "                                # resize, normalize and save the frame (convert to grayscale if frames_resolution[2] == 1)\n",
    "                                if(frames_resolution[2] == 1):\n",
    "                                    detected_face = cv2.cvtColor(detected_face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                                # detected_face = cv2.resize(detected_face, (frames_resolution[0], frames_resolution[1]))\n",
    "                                detected_face = cv2.normalize(detected_face, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "                                images.append(detected_face)\n",
    "\n",
    "                                # skip 'skip' next frames\n",
    "                                index += skip\n",
    "\n",
    "                    index += 1\n",
    "\n",
    "                    # if True, saves array of 'frames_count' images\n",
    "                    if len(images) == frames_count:\n",
    "                        item = {\"emotion\": np.asarray(toOneHot(emotion, emotions_count)), \"images\": np.asarray(images)}\n",
    "                        images = []\n",
    "                        array_count += 1\n",
    "                        emotions[emotion] += 1\n",
    "                        data.append(item)\n",
    "\n",
    "                    if (array_count >= max_array_count):\n",
    "                        break\n",
    "                        \n",
    "                update_progress(i/len(frames_folders))\n",
    "                print(\"Processed: {}/{} \\tAdded: \\tEmotion: {} \\t{} arrays of {} images\".format(i, len(frames_folders), emotion, array_count, frames_count))\n",
    "        \n",
    "            else:\n",
    "                update_progress(i/len(frames_folders))\n",
    "                print(\"Processed: {}/{} \\tEmotion: {} \\tAlready enough data of this emotion\".format(i, len(frames_folders), emotion)) \n",
    "\n",
    "        else:\n",
    "            update_progress(i/len(frames_folders))\n",
    "            print(\"Directory doesn't exist or contain not enough frames\") \n",
    "\n",
    "    update_progress(1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################################################################################################] 100.0%\n"
     ]
    }
   ],
   "source": [
    "test_data = get_data(test_frames_folder, test_save_folder)\n",
    "with open(os.path.join(project_directory,\"dataset\\\\resolution_{}x{}x{}_test.pickle\".format(frames_resolution[0], frames_resolution[1], frames_resolution[2])), 'wb') as handle:\n",
    "    pickle.dump(test_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = get_data(training_frames_folder, training_save_folder)\n",
    "with open(os.path.join(project_directory,\"dataset\\\\resolution_{}x{}x{}_train.pickle\".format(frames_resolution[0], frames_resolution[1], frames_resolution[2])), 'wb') as handle:\n",
    "    pickle.dump(train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show data distribution by emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:\n",
      "[279. 600. 568. 575. 611. 563. 626. 556.]\n",
      "\n",
      "test data:\n",
      "[12. 25. 24. 24. 28. 26. 28. 24.]\n"
     ]
    }
   ],
   "source": [
    "def data_distibution(data):\n",
    "    emotions_data = np.array([x[\"emotion\"] for x in data])\n",
    "    emotions = np.zeros(emotions_count)\n",
    "\n",
    "    for i, x in enumerate(emotions_data):\n",
    "        index = np.where(x == 1)[0][0]\n",
    "        emotions[index] += 1\n",
    "\n",
    "    return emotions\n",
    "    \n",
    "print(\"train data:\")\n",
    "print(data_distibution(train_data))\n",
    "\n",
    "print(\"\\ntest data:\")\n",
    "print(data_distibution(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append flipped images for emotion number 'emotion_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:\n",
      "[558. 600. 568. 575. 611. 563. 626. 556.]\n",
      "\n",
      "test data:\n",
      "[24. 25. 24. 24. 28. 26. 28. 24.]\n"
     ]
    }
   ],
   "source": [
    "def append_flipped(data, emotion_index):\n",
    "    for item in list(data):\n",
    "        em_index = np.where(item[\"emotion\"] == 1)[0][0]\n",
    "\n",
    "        if em_index == emotion_index:\n",
    "            arr = []     \n",
    "            for image in item[\"images\"]:\n",
    "                arr.append(cv2.flip(image, 0))\n",
    "\n",
    "            arr = np.asarray(arr)\n",
    "            new_item = {\"emotion\": item[\"emotion\"], \"images\": arr}\n",
    "            data.append(new_item)\n",
    "\n",
    "append_flipped(train_data, 0)\n",
    "append_flipped(test_data, 0)\n",
    "\n",
    "print(\"train data:\")\n",
    "print(data_distibution(train_data))\n",
    "\n",
    "print(\"\\ntest data:\")\n",
    "print(data_distibution(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append stretched and rotated images for emotion number 'emotion_index' (64x64 images) (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for images 64x64\n",
    "def append_stretched_rotated(data, emotion_index):\n",
    "    for item in list(data):\n",
    "        em_index = np.where(item[\"emotion\"] == 1)[0][0]\n",
    "\n",
    "        if em_index == emotion_index:\n",
    "            arr1 = []   \n",
    "            arr2 = [] \n",
    "            for image in item[\"images\"]:\n",
    "\n",
    "                # stretch and resize\n",
    "                arr1.append(cv2.resize(image, (64, 80), interpolation = cv2.INTER_AREA)[10:74, 0:64])\n",
    "\n",
    "                # rotate \n",
    "                M = cv2.getRotationMatrix2D((cols/2,rows/2),-10,1)\n",
    "                dst = cv2.warpAffine(image,M,(64,64), borderValue=(255,255,255))\n",
    "                arr2.append(dst)\n",
    "\n",
    "            arr1 = np.asarray(arr1)\n",
    "            arr2 = np.asarray(arr2)\n",
    "            new_item1 = {\"emotion\": item[\"emotion\"], \"images\": arr1}\n",
    "            new_item2 = {\"emotion\": item[\"emotion\"], \"images\": arr2}\n",
    "            data.append(new_item1)\n",
    "            data.append(new_item2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete data for emotion number 'emotion_index' (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_emotion_data(data, emotion_index):\n",
    "    data[:] = [value for value in data if (np.where(value[\"emotion\"] == 1)[0][0] != emotion_index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    file_name = os.path.join(project_directory,\"dataset//{}\".format(file_name))\n",
    "    with open(file_name, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and save train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data distribution:  [24. 24. 24. 24. 24. 24. 24. 24.]\n",
      "data distribution:  [500. 500. 500. 500. 500. 500. 500. 500.]\n"
     ]
    }
   ],
   "source": [
    "def prepare_data_4_save(data, max_instances_count):  \n",
    "    shuffled_data = np.asarray(data)\n",
    "    np.random.shuffle(shuffled_data)\n",
    "    \n",
    "    result = []\n",
    "    emotions = np.zeros(emotions_count)\n",
    "    emotions_data = np.array([x[\"emotion\"] for x in shuffled_data])\n",
    "\n",
    "    for j in range(0, len(shuffled_data)):  \n",
    "        index = np.where(emotions_data[j] == 1)[0][0]\n",
    "\n",
    "        if emotions[index] < max_instances_count:\n",
    "            emotion = np.zeros(emotions_count)\n",
    "            emotion[index] = 1\n",
    "            sample = shuffled_data[j]\n",
    "            sample[\"emotion\"] = emotion\n",
    "            result.append(sample)\n",
    "            emotions[index] += 1\n",
    "\n",
    "    print(\"data distribution: \", emotions)\n",
    "    return result\n",
    "\n",
    "def save_data(data, file_name):\n",
    "    file_name = os.path.join(project_directory,\"dataset//{}\".format(file_name))\n",
    "    with open(file_name, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "test_data_count = 24\n",
    "train_data_count = 500\n",
    "\n",
    "# form\n",
    "test_data_4_save = prepare_data_4_save(test_data, test_data_count)\n",
    "train_data_4_save = prepare_data_4_save(train_data, train_data_count)\n",
    "\n",
    "# save\n",
    "save_data(test_data_4_save, \"resolution_{}x{}x{}_count_{}_test_data.pickle\".format(frames_resolution[0], frames_resolution[1], frames_resolution[2], test_data_count))\n",
    "save_data(train_data_4_save, \"resolution_{}x{}x{}_count_{}_train_data.pickle\".format(frames_resolution[0], frames_resolution[1], frames_resolution[2], train_data_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "570px",
    "left": "1109px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
